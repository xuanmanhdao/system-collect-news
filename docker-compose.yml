version: '3.8'

services:
  # 1. Zookeeper
  zookeeper:
    image: bitnami/zookeeper:latest
    container_name: zookeeper
    ports:
      - "2181:2181"
    environment:
      - ALLOW_ANONYMOUS_LOGIN=yes
    healthcheck:
      test: ["CMD-SHELL", "echo ruok > /dev/tcp/localhost/2181; sleep 1; exit 0"]
      interval: 10s
      timeout: 5s
      retries: 5

  # 2. Kafka
  kafka:
    image: bitnami/kafka:3.4.0-debian-11-r0
    container_name: kafka
    ports:
      - "9092:9092"  # Cổng cho PLAINTEXT (nội bộ Docker)
      - "9093:9093"  # Cổng cho EXTERNAL (truy cập từ host hoặc bên ngoài)
    environment:
      - KAFKA_CFG_ZOOKEEPER_CONNECT=zookeeper:2181
      - KAFKA_CFG_LISTENERS=PLAINTEXT://0.0.0.0:9092,EXTERNAL://0.0.0.0:9093
      - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092,EXTERNAL://localhost:9093
      - KAFKA_CFG_AUTO_CREATE_TOPICS_ENABLE=false
      - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=PLAINTEXT:PLAINTEXT,EXTERNAL:PLAINTEXT
      - KAFKA_INTER_BROKER_LISTENER_NAME=PLAINTEXT
      - ALLOW_PLAINTEXT_LISTENER=yes
      - KAFKA_CFG_LOG4J_LOGGERS=kafka=DEBUG  # Bật debug log
      - KAFKA_CFG_MESSAGE_MAX_BYTES=2097152  # Hỗ trợ message 2MB
      - KAFKA_CFG_NUM_PARTITIONS=3
    depends_on:
      zookeeper:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "kafka-topics.sh --bootstrap-server kafka:9092 --list || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5
    deploy:
      resources:
        limits:
          cpus: '2.0'  # Tăng CPU
          memory: 2G  # Tăng RAM

  # 3. Kafka init (tạo topic)
  # kafka-init:
  #   image: bitnami/kafka:3.4.0-debian-11-r0
  #   container_name: kafka-init
  #   depends_on:
  #     kafka:
  #       condition: service_healthy
  #   volumes:
  #     - ./init-kafka.sh:/app/init-kafka.sh
  #   entrypoint: ["/bin/bash", "-c", "/app/init-kafka.sh"]

  # 4. Kafdrop (UI xem Kafka)
  kafdrop:
    image: obsidiandynamics/kafdrop
    container_name: kafdrop
    ports:
      - "9000:9000"
    environment:
      - KAFKA_BROKERCONNECT=kafka:9092
    depends_on:
      kafka:
        condition: service_healthy

  # 5. Redis
  redis:
    image: redis:6.2
    container_name: redis
    ports:
      - "6379:6379"
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s
      timeout: 3s
      retries: 5
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 1G

  # 6. Elasticsearch
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.11.1
    container_name: elasticsearch
    ports:
      - "9200:9200"
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=false
      - xpack.security.transport.ssl.enabled=false
    volumes:
      - esdata:/usr/share/elasticsearch/data
    healthcheck:
      test: ["CMD-SHELL", "curl -s http://localhost:9200/_cluster/health | grep -vq '\"status\":\"red\"'"]
      interval: 10s
      timeout: 5s
      retries: 5

  # 7. Kibana
  kibana:
    image: docker.elastic.co/kibana/kibana:8.11.1
    container_name: kibana
    ports:
      - "5601:5601"
    environment:
      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200
      - xpack.security.enabled=false
    depends_on:
      elasticsearch:
        condition: service_healthy

  # 8. Spark
  spark:
    image: bitnami/spark:3.4
    container_name: spark
    environment:
      - SPARK_MODE=client
    depends_on:
      kafka:
        condition: service_healthy
      elasticsearch:
        condition: service_healthy
    volumes:
      - ./processor-service:/opt/spark-apps
    command: >
      bash -c "
        /opt/bitnami/spark/bin/spark-submit \
        --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.3,org.elasticsearch:elasticsearch-spark-30_2.12:8.11.1 \
        /opt/spark-apps/spark_news_processor.py
      "

  # 9. Crawler
  crawler:
    build:
      context: ./crawler-service
    container_name: crawler
    depends_on:
      kafka:
        condition: service_healthy
      redis:
        condition: service_healthy
    volumes:
      - ./crawler-service:/app
      - ./crawler-service/logs:/var/log
    command: cron -f  # Update: start cron thay vì python main.py

volumes:
  esdata: