from pyspark.sql import SparkSession
from pyspark.sql.functions import from_json, col
from pyspark.sql.types import StructType, StringType
import logging

# 1. Setup logger
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("SparkNewsProcessor")

# 2. Kh·ªüi t·∫°o SparkSession,
# spark.sql.shuffle.partitions gi·∫£m s·ªë l∆∞·ª£ng shuffle partition n·∫øu nh·ªè, spark.streaming.backpressure.enabled b·∫≠t c∆° ch·∫ø ch·ªëng b√£o
spark = SparkSession.builder \
    .appName("NewsKafkaConsumer") \
    .config("spark.sql.shuffle.partitions", "3") \
    .config("spark.streaming.backpressure.enabled", "true") \
    .getOrCreate()

spark.sparkContext.setLogLevel("WARN")
logger.info("‚úÖ SparkSession started.")

# 3. ƒê·ªçc d·ªØ li·ªáu t·ª´ Kafka
try:
    df_kafka_raw = spark.readStream.format("kafka") \
        .option("kafka.bootstrap.servers", "kafka:9092") \
        .option("subscribe", "raw-news") \
        .option("startingOffsets", "earliest") \
        .option("failOnDataLoss", "false") \
        .option("checkpointLocation", "/tmp/checkpoint-kafka") \
        .load()
    logger.info("‚úÖ Connected to Kafka topic: raw-news")
except Exception as e:
    logger.error(f"‚ùå Failed to connect to Kafka: {e}", exc_info=True)
    raise

# 4. ƒê·ªãnh nghƒ©a schema cho JSON
json_schema = StructType() \
    .add("source", StringType()) \
    .add("url", StringType()) \
    .add("title", StringType()) \
    .add("image", StringType()) \
    .add("body", StringType())

# 5. Parse JSON t·ª´ Kafka value
df_parsed = df_kafka_raw.selectExpr("CAST(value AS STRING)") \
    .select(from_json(col("value"), json_schema).alias("data")) \
    .select("data.*")

logger.info("‚úÖ Schema parsed from Kafka:")
df_parsed.printSchema()

# 6. Ghi d·ªØ li·ªáu v√†o Elasticsearch
try:
    query = df_parsed.writeStream \
        .outputMode("append") \
        .format("org.elasticsearch.spark.sql") \
        .option("checkpointLocation", "/tmp/checkpoint-es") \
        .option("es.nodes", "elasticsearch") \
        .option("es.port", "9200") \
        .option("es.resource", "news") \
        .option("es.nodes.wan.only", "false") \
        .option("es.mapping.id", "url") \
        .start()

    logger.info("üöÄ Streaming started: Kafka -> Elasticsearch")
    query.awaitTermination()

except Exception as e:
    logger.error(f"‚ùå Failed to start streaming to Elasticsearch: {e}", exc_info=True)
    raise