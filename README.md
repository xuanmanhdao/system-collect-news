# ğŸ“š System Collect News - README
## ğŸ§© Tá»•ng Quan
Dá»± Ã¡n System Collect News lÃ  má»™t há»‡ thá»‘ng crawler tá»± Ä‘á»™ng Ä‘a nguá»“n (VnExpress, Tuá»•i Tráº», ...), thu tháº­p tin tá»©c vá» Kafka, xá»­ lÃ½ dá»¯ liá»‡u vá»›i Spark vÃ  lÆ°u trá»¯ vÃ o Elasticsearch.
Há»‡ thá»‘ng thiáº¿t káº¿ microservices cháº¡y báº±ng Docker Ä‘á»ƒ Ä‘áº£m báº£o kháº£ nÄƒng scale, tá»‘i Æ°u vÃ  dá»… dÃ ng maintain.

## ğŸ›  ThÃ nh pháº§n chÃ­nh

| Service             | MÃ´ táº£                                               |
|---------------------|-----------------------------------------------------|
| **Zookeeper**       | Quáº£n lÃ½ cluster Kafka                               |
| **Kafka**           | Message broker trung tÃ¢m                            |
| **Kafdrop**         | UI web quáº£n lÃ½ Kafka                                |
| **Redis**           | Cache chá»‘ng duplicate bÃ i viáº¿t                      |
| **Elasticsearch**   | LÆ°u trá»¯ dá»¯ liá»‡u tin tá»©c dáº¡ng document               |
| **Kibana**          | Giao diá»‡n search/visualize dá»¯ liá»‡u tá»« Elasticsearch |
| **Spark**           | Xá»­ lÃ½ dá»¯ liá»‡u Kafka trÆ°á»›c khi Ä‘Æ°a vÃ o Elasticsearch |
| **Crawler Service** | Thu tháº­p dá»¯ liá»‡u tá»« cÃ¡c bÃ¡o Ä‘iá»‡n tá»­                 |

## ğŸš€ CÃ¡ch cháº¡y há»‡ thá»‘ng
### 1. Clone project
```
git clone https://github.com/yourname/system-collect-news.git
cd system-collect-news/source-code
```
### 2. Cháº¡y mÃ´i trÆ°á»ng DEV
```
# Báº¯t Ä‘áº§u services mÃ´i trÆ°á»ng dev
./start-dev.sh
```
Crawler service sáº½ cháº¡y vÃ  push tin tá»©c vÃ o Kafka.

Kafdrop má»Ÿ táº¡i: http://localhost:9000

Elasticsearch táº¡i: http://localhost:9200

Kibana táº¡i: http://localhost:5601

Ghi chÃº: Dev máº·c Ä‘á»‹nh cháº¡y cron má»—i 1 phÃºt, dá»… test.

### 3. Cháº¡y mÃ´i trÆ°á»ng PROD
```
# Báº¯t Ä‘áº§u services mÃ´i trÆ°á»ng prod
./start-prod.sh
```
Prod cáº¥u hÃ¬nh tá»‘i Æ°u hÆ¡n (log chuáº©n hÃ³a, crontab phÃ¢n chia rÃµ).

### 4. CÃ¡c lá»‡nh quáº£n lÃ½ khÃ¡c
```
# Stop táº¥t cáº£ containers
./stop-all.sh

# Restart nhanh mÃ´i trÆ°á»ng dev
./restart-dev.sh

# Restart nhanh mÃ´i trÆ°á»ng prod
./restart-prod.sh
```
## ğŸ§© Cáº¥u trÃºc thÆ° má»¥c
```
source-code/
â”‚
â”œâ”€â”€ crawler-service/
â”‚   â”œâ”€â”€ crontab.template.txt
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ crawler/
â”‚       â”œâ”€â”€ spiders/
â”‚           â”œâ”€â”€ vnexpress_spider.py
â”‚           â”œâ”€â”€ tuoitre_spider.py
â”‚       â”œâ”€â”€ utils/
â”‚           â”œâ”€â”€ kafka_producer.py
â”‚           â”œâ”€â”€ redis_client.py
â”‚           â”œâ”€â”€ config.py
â”‚           â”œâ”€â”€ healthcheck.py
â”‚   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ processor-service/    # (Xá»­ lÃ½ Spark, náº¿u cáº§n)
â”‚   â”œâ”€â”€ spark_news_processor.py
â”‚
â”œâ”€â”€ news-service/    # (Expose Restful API)
â”‚
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ docker-compose.override.yml
â”œâ”€â”€ .env.dev
â”œâ”€â”€ .env.prod
â”œâ”€â”€ start-dev.sh
â”œâ”€â”€ start-prod.sh
â”œâ”€â”€ stop-and-clean-dev.sh
â”œâ”€â”€ stop-and-clean-prod.sh
â”œâ”€â”€ restart-dev.sh
â”œâ”€â”€ restart-prod.sh
â””â”€â”€ README.md  # (file nÃ y)
```
### âš™ï¸ TÃ¹y chá»‰nh cáº¥u hÃ¬nh
Báº¡n cÃ³ thá»ƒ thay Ä‘á»•i file:
.env.dev
.env.prod
Ä‘á»ƒ config:

| Key                      | Ã nghÄ©a                               |
|--------------------------|---------------------------------------|
| **KAFKA_HOST**           | Äá»‹a chá»‰ kafka broker                  |
| **KAFKA_PORT**           | Port kafka                            |
| **REDIS_HOST**           | Äá»‹a chá»‰ redis                         |
| **REDIS_PORT**           | Port redis                            |
| **Elasticsearch**        | LÆ°u trá»¯ dá»¯ liá»‡u tin tá»©c dáº¡ng document |
| **MAX_RETRIES**          | Sá»‘ láº§n retry connect Kafka/Redis      |
| **RETRY_DELAY_SECONDS**  | Khoáº£ng delay giá»¯a cÃ¡c láº§n retry       |

## â³ Há»‡ thá»‘ng cháº¡y nhÆ° tháº¿ nÃ o?
crawler-service dÃ¹ng cron Ä‘á»ƒ cháº¡y Ä‘á»‹nh ká»³ má»—i phÃºt vÃ  báº¯n dá»¯ liá»‡u lÃªn Kafka topic raw-news.

spark Ä‘á»c dá»¯ liá»‡u Kafka, lÃ m sáº¡ch, enrich vÃ  ghi xuá»‘ng Elasticsearch.

kafdrop há»— trá»£ xem topic Kafka realtime.

## ğŸ“… Cáº¥u hÃ¬nh lá»‹ch crawl (crawler-service/crontab.txt)
```
* * * * * cd /app && python main.py vnexpress 2>&1 | tee -a /var/log/vnexpress_$(date +\%Y\%m\%d\%H\%M\%S).log
* * * * * cd /app && python main.py tuoitre 2>&1 | tee -a /var/log/tuoitre_$(date +\%Y\%m\%d\%H\%M\%S).log
```

Cá»© má»—i phÃºt, crawler tá»± Ä‘á»™ng láº¥y bÃ i má»›i vÃ  lÆ°u log theo timestamp má»›i Ä‘á»ƒ dá»… quáº£n lÃ½.

## ğŸ”¥ Má»™t sá»‘ ghi chÃº quan trá»ng
Topic Kafka Ä‘Æ°á»£c tá»± Ä‘á»™ng táº¡o náº¿u chÆ°a tá»“n táº¡i (3 partition, replication 1).

Redis dÃ¹ng kiá»ƒm tra duplicate bÃ i viáº¿t theo hash SHA256.

Elasticsearch lÆ°u tin tá»©c theo chuáº©n schema cÃ³ source, title, body, image, url.

Spark há»— trá»£ má»Ÿ rá»™ng batch hoáº·c micro-batch Ä‘á»ƒ tá»‘i Æ°u tá»‘c Ä‘á»™ xá»­ lÃ½ lá»›n hÆ¡n.

Náº¿u cáº§n thÃªm website crawler â†’ chá»‰ cáº§n thÃªm Spider má»›i á»Ÿ crawler/spiders/ vÃ  khai bÃ¡o main.py.

## ğŸ› ï¸ TODO trong tÆ°Æ¡ng lai
 ThÃªm há»‡ thá»‘ng Prometheus + Grafana Ä‘á»ƒ giÃ¡m sÃ¡t.

 Má»Ÿ rá»™ng sang crawl thÃªm cÃ¡c bÃ¡o khÃ¡c (Vietnamnet, Zingnews...).

 ThÃªm chá»©c nÄƒng gá»­i alert khi gáº·p lá»—i lá»›n (Kafka consumer lag, Spark job failed).

 Build UI phÃ¢n tÃ­ch dá»¯ liá»‡u Ä‘Ã£ crawl (Top trending, category classification).
 
 TÃ­ch há»£p thÃªm lÆ°u backup dá»¯ liá»‡u tin tá»©c ra file JSON hoáº·c Database.
 
 CÃ¢n nháº¯c scale multiple crawler container.



